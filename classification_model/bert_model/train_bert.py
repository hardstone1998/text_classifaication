import os

import torch
from torch.utils.data import DataLoader
from transformers import BertTokenizer, BertForSequenceClassification, AdamW

from transformers import get_scheduler
from tqdm import tqdm

from utils.load_config import load_config

base_model_name = "bert-base-chinese"
config = load_config()
save_models = config.get('save_models')
bert_dir = "bert_dir"


def train_and_save_bert_model(train_loader, model_name, device):
    tokenizer = BertTokenizer.from_pretrained(base_model_name)
    model = BertForSequenceClassification.from_pretrained(base_model_name, num_labels=2).to(device)

    # ä¼˜åŒ–å™¨ & å­¦ä¹ ç‡è°ƒåº¦å™¨
    optimizer = AdamW(model.parameters(), lr=2e-5)
    lr_scheduler = get_scheduler("linear", optimizer=optimizer,
                                 num_warmup_steps=0,
                                 num_training_steps=len(train_loader) * 3)

    # ğŸ” å¼€å§‹è®­ç»ƒ
    num_epochs = 16
    model.train()
    for epoch in range(num_epochs):
        loop = tqdm(train_loader, desc=f"Epoch {epoch + 1}")
        for batch in loop:
            batch = {k: v.to(device) for k, v in batch.items()}

            outputs = model(input_ids=batch["input_ids"],
                            attention_mask=batch["attention_mask"],
                            labels=batch["label"])

            loss = outputs.loss
            loss.backward()

            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()

            loop.set_postfix(loss=loss.item())

    save_bert_full_dir = os.path.join(save_models, "embedding_dir")
    os.makedirs(save_bert_full_dir, exist_ok=True)

    # âœ… ä¿å­˜æ¨¡å‹å’Œ tokenizer
    model_save_path = os.path.join(save_bert_full_dir, "bert", model_name)
    os.makedirs(model_save_path, exist_ok=True)

    model.save_pretrained(model_save_path)
    tokenizer.save_pretrained(model_save_path)

    print(f"æ¨¡å‹å’Œåˆ†è¯å™¨å·²ä¿å­˜åˆ°: {model_save_path}")

    return tokenizer, model


def load_bert_model(model_name, device):
    save_bert_full_dir = os.path.join(save_models, "embedding_dir")
    os.makedirs(save_bert_full_dir, exist_ok=True)
    model_save_path = os.path.join(save_bert_full_dir, "bert", model_name)
    os.makedirs(model_save_path, exist_ok=True)
    # æ‹¼æ¥ä¿å­˜è·¯å¾„
    model_path = os.path.join(save_bert_full_dir, "bert", model_name)

    # åˆ¤æ–­è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path, device):
        return None, None

    # åŠ è½½ tokenizer å’Œ model
    tokenizer = BertTokenizer.from_pretrained(model_path)
    model = BertForSequenceClassification.from_pretrained(model_path).to(device)

    print(f"æ¨¡å‹å’Œåˆ†è¯å™¨å·²ä» {model_path} åŠ è½½æˆåŠŸ")
    return tokenizer, model
